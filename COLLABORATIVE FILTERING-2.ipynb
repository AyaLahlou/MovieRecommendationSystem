{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ayalahlou/Final Project 001/ml-100k\n"
     ]
    }
   ],
   "source": [
    "cd ml-100k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('u.data', sep='\\t', names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COLLABORATIVE FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering can be divided into 2 categories:\n",
    "    * Memory Based Approach: this technique find similar users based on cosine similarity and pearson correlation and take weighted average of ratings.\n",
    "    \n",
    "    * Model Based Approach: this technique uses Machine LEarning to find user ratings of unrated items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of our final project, we will be using Memory Based Collaborative Filtering, more precisely user-item filtering as we are interested in taking a particular user, find users that are similar to that user based on similarity of ratings, and recommend movies that those similar users liked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will first build the User-Movie matrix.\n",
    "\n",
    "Whenever a User X has rated a movie Y, the corresponding cell would have the value of the rating User X gave. If User X did not rate a movie Z, the corresponding cell would be filled as 0. However, we shouldn't consider the missing ratings to be truly zero but rather just as empty entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall split our data into a training set and a testing set by removing 10 ratings per user from the training set and placing them in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], size=10, replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test\n",
    "train, test = split(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common distance metric of user-item similarity is cosine similarity. The metric can be thought of geometrically if one treats a given user's row of the ratings matrix as a vector. For user-based collaborative filtering, two users' similarity is measured as the cosine of the angle between the two users' vectors. For users ${u}$ and ${u'}$, the cosine similarity is\n",
    "\n",
    "$$ sim(u, u') = cos(\\theta{}) = \\frac{\\textbf{r}_{u} \\dot{} \\textbf{r}_{u'}}{\\| \\textbf{r}_{u} \\| \\| \\textbf{r}_{u'} \\|} = \\sum_{i} \\frac{r_{ui}r_{u'i}}{\\sqrt{\\sum\\limits_{i} r_{ui}^2} \\sqrt{\\sum\\limits_{i} r_{u'i}^2} } $$\n",
    "\n",
    "The cosine similarity will range from 0 to 1 because there are no negative ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityfunc(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T) + epsilon\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = similarityfunc(train, kind='user')\n",
    "item_similarity = similarityfunc(train, kind='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the User-Movie Matrix, we can predict the ratings that were not included with the data. \n",
    "\n",
    "Using these predictions, we can then compare them with the test data to eavluate to validate the quality of our recommender model.\n",
    "\n",
    "For user-based collaborative filtering, we predict that a user's $u$'s rating for item $i$ is given by the sum of all other users' ratings for the same item $i$ where the weighting is the cosine similarity between the each user and the input user $u$.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\sum\\limits_{u'}sim(u, u') r_{u'i}$$\n",
    "We must also normalize by the number of ${r_{u'i}}$ ratings:\n",
    "$$\\hat{r}_{ui} = \\frac{\\sum\\limits_{u'} sim(u, u') r_{u'i}}{\\sum\\limits_{u'}|sim(u, u')|}$$\n",
    "As with before, our computational speed will benefit greatly by favoring NumPy functions over for loops. With our slow function below, even though I use NumPy methods, the presence of the for-loop still slows the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif kind == 'item':\n",
    "        return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the scikit-learn’s mean squared error function as our validation metric. Comparing user- and item-based collaborative filtering, it looks like user-based collaborative filtering gives us a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 8.453780443912272\n",
      "Item-based CF MSE: 11.568891782031098\n"
     ]
    }
   ],
   "source": [
    "item_prediction = predict(train, item_similarity, kind='item')\n",
    "user_prediction = predict(train, user_similarity, kind='user')\n",
    "\n",
    "print ('User-based CF MSE: ' + str(get_mse(user_prediction, test)))\n",
    "print ('Item-based CF MSE: ' + str(get_mse(item_prediction, test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-$k$ Collaborative Filtering\n",
    "\n",
    "We can attempt to improve our prediction MSE by only considering the top $k$ users who are most similar to the input user (or, similarly, the top $k$ items). That is, when we calculate the sums over $u'$\n",
    "$$\\hat{r}_{ui} = \\frac{\\sum\\limits_{u'} sim(u, u') r_{u'i}}{\\sum\\limits_{u'}|sim(u, u')|}$$\n",
    "we only sum over the top $k$ most similar users. A slow implementation of this algorithm is shown below. While I am sure that there is a way to use numpy sorting to get rid of the double for-loops, I got pretty frustrated desciphering the 2D argsort output and just settled for the slow loop.\n",
    "As is shown below, employing this method actually halves our error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk(ratings, similarity, kind='user', k=40):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        for i in range(ratings.shape[0]):\n",
    "            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))\n",
    "    if kind == 'item':\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items]))        \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pred = predict_topk(train, user_similarity, kind='user', k=40)\\nprint ('Top-k User-based CF MSE: ' + str(get_mse(pred, test)))\\n\\npred = predict_topk(train, item_similarity, kind='item', k=40)\\nprint ('Top-k Item-based CF MSE: ' + str(get_mse(pred, test)))\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pred = predict_topk(train, user_similarity, kind='user', k=40)\n",
    "print ('Top-k User-based CF MSE: ' + str(get_mse(pred, test)))\n",
    "\n",
    "pred = predict_topk(train, item_similarity, kind='item', k=40)\n",
    "print ('Top-k Item-based CF MSE: ' + str(get_mse(pred, test)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_movie = {}\n",
    "a=[]\n",
    "with open('u.item', 'r',errors='replace') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        info[1]= info[1].split(\" (\")[0]\n",
    "        idx_to_movie[int(info[0])-1] = info[1]\n",
    "        a.append(info)\n",
    "def top_k_movies(similarity, mapper, movie_idx, k=10):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Toy Story',\n",
       " 'Star Wars',\n",
       " 'Return of the Jedi',\n",
       " 'Independence Day',\n",
       " 'Raiders of the Lost Ark',\n",
       " 'Rock, The',\n",
       " 'Star Trek: First Contact',\n",
       " 'Mission: Impossible',\n",
       " 'Willy Wonka and the Chocolate Factory',\n",
       " 'Empire Strikes Back, The']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fav='Toy Story'\n",
    "for i in range(1682):\n",
    "    if a[i][1]==fav:\n",
    "        idx=i\n",
    "        break\n",
    "movies = top_k_movies(item_similarity, idx_to_movie, idx)\n",
    "display(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Story (link not available)\n"
     ]
    }
   ],
   "source": [
    "li=[]\n",
    "try:\n",
    "    \n",
    "    for j in range(len(movies)):\n",
    "        for i in range(len(mov)):\n",
    "            #if movies[j]=='Batman':\n",
    "                #reclink='https://www.imdb.com/title/'+'tt0096895'+'/?ref_=fn_al_tt_1'\n",
    "                #li.append(reclink)\n",
    "                #break\n",
    "            if movies[j]==mov['original_title'][i]:\n",
    "                reclink='https://www.imdb.com/title/'+mov['imdb_id'][i]+'/?ref_=fn_al_tt_1'\n",
    "                li.append(reclink)\n",
    "                break\n",
    "            elif movies[j]!=mov['original_title'][i] and i==45465:\n",
    "                reclink='(link not available)'\n",
    "                li.append(reclink)\n",
    "                break   \n",
    "    for o in range(10):\n",
    "        if o==0: \n",
    "            print ('Users who liked', movies[o],'also liked: \\n ' )\n",
    "        else:\n",
    "            print('*',movies[o], li[o])\n",
    "except:\n",
    "    print(movies[i], '(link not available)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEARSON CORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that could be the issue is that very popular movies like Star Wars are being favored. We can remove some of this bias by considering a different similarity metric - the pearson correlation. I’ll just grab the built-in scikit-learn function for computing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "item_correlation = 1 - pairwise_distances(train.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_to_movie = {}\n",
    "a=[]\n",
    "with open('u.item', 'r',errors='replace') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        info[1]= info[1].split(\" (\")[0]\n",
    "        idx_to_movie[int(info[0])-1] = info[1]\n",
    "        a.append(info)\n",
    "    \n",
    "def top_k_movies(similarity, mapper, movie_idx, k=10):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Snow White and the Seven Dwarfs',\n",
       " 'Cinderella',\n",
       " 'Pinocchio',\n",
       " 'Beauty and the Beast',\n",
       " 'Dumbo',\n",
       " 'Fantasia',\n",
       " 'Mary Poppins',\n",
       " 'Aladdin',\n",
       " 'Lion King, The',\n",
       " 'Alice in Wonderland']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fav='Snow White and the Seven Dwarfs'\n",
    "for i in range(1682):\n",
    "    if a[i][1]==fav:\n",
    "        idx=i\n",
    "        break\n",
    "movies = top_k_movies(item_correlation, idx_to_movie, idx)\n",
    "display(movies)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov=pd.read_csv('movies_metadata.csv')\n",
    "mov=mov.loc[:,'imdb_id':'original_title']\n",
    "mov=mov.drop(columns=['original_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users who liked Snow White and the Seven Dwarfs also liked: \n",
      " \n",
      "* Cinderella https://www.imdb.com/title/tt0042332/?ref_=fn_al_tt_1\n",
      "* Pinocchio https://www.imdb.com/title/tt0032910/?ref_=fn_al_tt_1\n",
      "* Beauty and the Beast https://www.imdb.com/title/tt0101414/?ref_=fn_al_tt_1\n",
      "* Dumbo https://www.imdb.com/title/tt0033563/?ref_=fn_al_tt_1\n",
      "* Fantasia https://www.imdb.com/title/tt0032455/?ref_=fn_al_tt_1\n",
      "* Mary Poppins https://www.imdb.com/title/tt0058331/?ref_=fn_al_tt_1\n",
      "* Aladdin https://www.imdb.com/title/tt0103639/?ref_=fn_al_tt_1\n",
      "* Lion King, The (link not available)\n",
      "* Alice in Wonderland https://www.imdb.com/title/tt0043274/?ref_=fn_al_tt_1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "li=[]\n",
    "try:\n",
    "    \n",
    "    for j in range(len(movies)):\n",
    "        for i in range(len(mov)):\n",
    "            #if movies[j]=='Batman':\n",
    "                #reclink='https://www.imdb.com/title/'+'tt0096895'+'/?ref_=fn_al_tt_1'\n",
    "                #li.append(reclink)\n",
    "                #break\n",
    "            if movies[j]==mov['original_title'][i]:\n",
    "                reclink='https://www.imdb.com/title/'+mov['imdb_id'][i]+'/?ref_=fn_al_tt_1'\n",
    "                li.append(reclink)\n",
    "                break\n",
    "            elif movies[j]!=mov['original_title'][i] and i==45465:\n",
    "                reclink='(link not available)'\n",
    "                li.append(reclink)\n",
    "                break   \n",
    "    for o in range(10):\n",
    "        if o==0: \n",
    "            print ('Users who liked', movies[o],'also liked: \\n ' )\n",
    "        else:\n",
    "            print('*',movies[o], li[o])\n",
    "except:\n",
    "    print(movies[o], '(link not available)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
